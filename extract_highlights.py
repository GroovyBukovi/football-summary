# -*- coding: utf-8 -*-
"""extract_highlights.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qTdMIIXI5E0Z0murJiiFXZNAAKe_N7VE
"""

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
from huggingface_hub import login

login("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")

# Load model & tokenizer
model_id = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

# Load transcript
with open("FULL MATCH_ Portugal v Spain _ 2018 FIFA World Cup.txt", "r", encoding="utf-8") as f:
    transcript = f.read()

print(len(transcript))

# Split transcript into chunks (if it's long)
# increase number of tokens if ypu have RAM etc. max_tokens=7000
def chunk_text(text, max_tokens=800):
    lines = text.splitlines()
    chunks, chunk = [], ""
    for line in lines:
        if len(tokenizer(chunk + line).input_ids) < max_tokens:
            chunk += line + "\n"
        else:
            chunks.append(chunk.strip())
            chunk = line + "\n"
    if chunk:
        chunks.append(chunk.strip())
    return chunks

chunks = chunk_text(transcript)

# improve the prompt to make instructions more detailed and clear
def make_prompt(chunk):
    return f"""### Instruction:
You are a sports analyst. Extract the most important football match highlights from this transcript.

Only include:
- Goals
- Red/yellow cards
- Penalty kicks or missed chances
- Injuries
- Important substitutions

Include timestamps and short descriptions.

### Transcript:
{chunk}

### Highlights:"""

# Use a pipeline for inference
# try running with max_new_tokens=512 first, and if you notice truncated results, go for max_new_tokens=1024.
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=512)

# Process all chunks
all_highlights = []
for i, chunk in enumerate(chunks):
    prompt = make_prompt(chunk)
    num_tokens = len(tokenizer(prompt).input_ids)
    print(f"Prompt tokens (chunk {i+1}): {num_tokens}")


    print(f"Processing chunk {i+1}/{len(chunks)}...")
    out = pipe(prompt)[0]['generated_text']
    print(out)
    highlights = out.split("### Highlights:")[-1].strip()
    all_highlights.append(highlights)

# Save to file
with open("match_highlights.txt", "w", encoding="utf-8") as f:
    f.write("\n\n".join(all_highlights))